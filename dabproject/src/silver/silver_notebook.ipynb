{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfada88d-395a-4fac-9607-a7ea7b734b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_path = \"assertbundles.bronze.raw_employee_data\"\n",
    "raw_df = spark.read.table(raw_path)\n",
    "raw_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb2d26c8-0528-4dc7-bdb9-a714eaf08b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, lower\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# 1. Drop rows with nulls in essential columns (id, age, department, email)\n",
    "df_clean = raw_df.dropna(subset=[\"id\", \"age\", \"department\", \"email\"])\n",
    "\n",
    "# 2. Filter only rows where 'age' is numeric\n",
    "df_clean = df_clean.filter(col(\"age\").rlike(\"^[0-9]+$\"))\n",
    "\n",
    "# 3. Trim spaces and normalize string columns (name, department, email)\n",
    "df_clean = df_clean.withColumn(\"name\", trim(col(\"name\"))) \\\n",
    "                   .withColumn(\"department\", trim(col(\"department\"))) \\\n",
    "                   .withColumn(\"email\", trim(lower(col(\"email\"))))\n",
    "\n",
    "# 4. Validate email format with regex\n",
    "email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "df_clean = df_clean.filter(col(\"email\").rlike(email_regex))\n",
    "\n",
    "# 5. Cast age and salary to correct data types\n",
    "df_clean = df_clean.withColumn(\"age\", col(\"age\").cast(IntegerType())) \\\n",
    "                   .withColumn(\"salary\", col(\"salary\").cast(FloatType()))\n",
    "\n",
    "# 6. Write cleaned data into managed Spark table (overwrite if exists)\n",
    "df_clean.write.mode(\"overwrite\").saveAsTable(\"assertbundles.silver.clean_employee_data\")\n",
    "\n",
    "# Optional: Show the cleaned data\n",
    "df_clean.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
